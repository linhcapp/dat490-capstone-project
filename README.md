# ğŸˆ College Football Fraud Score: Detecting Overrated Teams with Machine Learning

## ğŸ“˜ Overview
This project develops a **data-driven framework** to identify *â€œfraudâ€* teams in college football â€” teams whose rankings and public perception exceed their on-field performance.  

The study was conducted as part of the **DAT 490 Capstone Project** at **Arizona State University** (Fall 2025).

---

## ğŸ§  Methodology

### 1. **Data Collection**
We combined data from two major public sources:
- [College Football Data API](https://collegefootballdata.com)  
- [Sports-Reference.com](https://sports-reference.com/cfb)

The merged dataset spans **2014â€“2024**, covering over **1000 team-seasons** with key metrics such as:
- Strength of Schedule (SOS)  
- Margin of Victory (MOV)  
- Simple Rating System (SRS)  
- Elo Ratings  
- AP Poll Rankings  

### 2. **Feature Engineering & Standardization**
- Standardized all numeric variables using **z-scores** within each season.  
- Created composite indices for:
  - **Perception** â†’ based on AP rankings and excitement metrics  
  - **Performance** â†’ based on efficiency and dominance metrics  
- Defined the **Fraud Score** as:  

  \[
  \text{Fraud Score} = \text{Perception} - \text{Performance}
  \]

  Positive values indicate *overrated (fraudulent)* teams; negative values indicate *underrated* teams.

### 3. **Unsupervised Learning (Clustering)**
- Applied **KMeans Clustering** (season by season) to group teams into performance tiers:
  - **Cluster 0:** Elite Contenders  
  - **Cluster 1:** Solid Performers  
  - **Cluster 2:** Underperformers / Frauds  
- Evaluated clustering quality using **Silhouette Scores** and **Inertia (Elbow) plots**.  
- Used **Principal Component Analysis (PCA)** for 2D visualization of clusters.

### 4. **Fraud Labeling & Classification**
- Constructed **empirical performance thresholds** based on the average of historical top-10 teams in each season.  
- Assigned teams as **â€œEliteâ€**, **â€œBorderlineâ€**, or **â€œFraudâ€** based on threshold comparisons and AP Rank deviation.  
- Trained a **Random Forest Classifier** using independent performance metrics (WinPct, SRS, MOV, SOS, Elo) to evaluate whether the fraud labeling aligns with actual outcomes.

---

## ğŸ§© Repository Structure

```
ğŸ“ CollegeFootball-FraudScore/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # Original CSVs from Sports Reference & CFBD API
â”‚   â”œâ”€â”€ processed/                # Cleaned & merged datasets (final_merged.csv)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_cleaning.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_clustering_pca.ipynb
â”‚   â”œâ”€â”€ 04_random_forest.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils.py                  # Helper functions for cleaning & feature creation
â”‚   â”œâ”€â”€ clustering.py             # KMeans + PCA visualization
â”‚   â”œâ”€â”€ modeling.py               # Random Forest classifier and metrics
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/                  # Plots: threshold charts, PCA, elbow curves
â”‚   â”œâ”€â”€ tables/                   # Cluster summaries and fraud score tables
â”‚
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ requirements.txt              # Dependencies list
â””â”€â”€ LICENSE
```

---

## ğŸ“Š Example Outputs

- **Elbow & Silhouette Plots:** Helped determine the optimal number of clusters (k â‰ˆ 3).  
- **PCA Visualization:** Showed clear separation between elite contenders, average teams, and underperformers.  
- **Random Forest Feature Importance:** Highlighted WinPct, MOV, and SRS as the strongest predictors of fraud labels.

---

## ğŸ§¾ Key Findings

- Fraud Scores successfully distinguished between **overvalued teams (e.g., Texas A&M 2019, Clemson 2022)** and **legitimate contenders (e.g., Ohio State 2014, LSU 2019)**.  
- Statistical validation showed strong alignment between **model predictions** and **expert consensus**.  
- The model generalizes across seasons, providing a foundation for automated ranking fairness audits.

---

## ğŸ§© Future Work
- Incorporate advanced efficiency metrics (e.g., **SP+**, **EPA/play**, **Massey Ratings**) for deeper contextual modeling.  
- Extend framework to real-time fraud detection during the ongoing season.  
- Compare machine-driven â€œFraud Scoresâ€ to human poll updates weekly to detect media overreactions.

---

## ğŸ‘¥ Contributors
- **Hai Ta Tuan** â€“ Data Engineering, Modeling, Report Writing  
- **Linh Luong**, **Khanh Nguyen**, **Nam Tran** â€“ EDA, Feature Engineering, Visualization

---

## ğŸ“š References
- [College Football Data API](https://collegefootballdata.com/about)  
- [Sports Reference](https://www.sports-reference.com/cfb/)  
- Coleman et al. (2010), *Journal of Sports Economics*, â€œVoter Bias in the Associated Press College Football Pollâ€  
- Stone & Rod (2016), *Marquette Sports Law Review*, â€œBias in the College Football Playoff Selection Processâ€
